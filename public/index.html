<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
    <script
    type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/1.3.6/socket.io.min.js"
  ></script>

  <script>
    var video = document.getElementById('videoElement')
    /*
      //uncomment this section to use webcam

      if (navigator.mediaDevices.getUserMedia) {
        navigator.mediaDevices
          .getUserMedia({ video: true })
          .then(function (stream) {
            video.srcObject = stream
          })
          .catch(function (error) {
            console.log('Something went wrong!')
          })
      }
    */



    // server URL and connect to server
    // CompnentDidMount()
    var server_url = 'http://www.mrexy.com:5000'
    var socket = io.connect(server_url, { transports: ['websocket'] });

    // you will recieve data on this end point, manupliate accordingly using canvas

    
    socket.on('detectedFace', function (data) {
      console.log('Detected an face...')
      console.log(data)
/*
      var canvas = document.getElementById('bb_canvas')
      var cw = canvas.width
      var ch = canvas.height
      canvas.width = 0
      canvas.height = 0 
      canvas.width = cw
      canvas.height = ch
      var w_fact = canvas.width / canvas.offsetWidth
      var h_fact = canvas.height / canvas.offsetHeight
      ctx = canvas.getContext('2d')
      console.log('data.faces:=>', typeof data.faces)
      console.log('data:=>', typeof data)
      /*
        [
          {x, y, w, h, color, name, probability, age, gender},
          {x, y, w, h, color, name, probability, age, gender},
          {x, y, w, h, color, name, probability, age, gender},
          {x, y, w, h, color, name, probability, age, gender},
        ]
      
      
     
      if (data.faces) {
        var node = document.createElement('LI') // Create a <li> node
        var text = `--------------------------------------------------`
        var textnode = document.createTextNode(text) // Create a text node
        node.appendChild(textnode) // Append the text to <li>
        document.getElementById('backendList').appendChild(node) // Append <li> to <ul> with id="myList"
        data.faces.forEach((face) => {
          ctx.strokeStyle = face.color
          ctx.rect(
            face.x * w_fact,
            face.y * h_fact,
            Math.abs(face.w - face.x) * w_fact,
            Math.abs(face.h - face.y) * h_fact
          )
          ctx.stroke()
          ctx.fillText(face.text, 10, 50)
          var node = document.createElement('LI') // Create a <li> node
          var text = `x=${face.x}, y=${face.y}, w=${face.w}, h=${face.h}, `
          var text =
            text + `probability=${face.probability}, name=${face.name}`
          var textnode = document.createTextNode(text) // Create a text node
          node.appendChild(textnode) // Append the text to <li>
          document.getElementById('backendList').appendChild(node) // Append <li> to <ul> with id="myList"
        })
      }

      // something similar thing needs to be done
      // just loop through the array you are getting
      // and display the data accordingly
      // like making bounding boxes as below
      // rn it is not working(may be I am looping incorrectly)
      // just rectify it in JS

      /*
      for (data in data.faces) {
        console.log('data=>', data)
        ctx.strokeStyle = data['color']
        ctx.rect(
          data['x'] * w_fact,
          data['y'] * h_fact,
          data['w'] * w_fact,
          data['h'] * h_fact
        )
        ctx.stroke()
        ctx.fillText(data['text'], 10, 50)
      }
      */
      //
    })



    // convert image to blob (will be same for most of the thing)
    function dataURLtoBlob(dataURL) {
      var binary = atob(dataURL.split(',')[1])
  var array=[]

  //setting bytes of array to correct value
  for(let i=0;i<binary.length;i++){
      array.push(binary.charCodeAt(i))
  }

  //Using Uint8Array() varructor to create array buffer
  return new Blob([new Uint8Array(array)], { type: 'image/png' })
    }

    // capture the video frame
    function captureImage() {
      var video = document.getElementById('videoElement')

      var canvas = document.createElement('canvas')
      canvas.width = video.offsetWidth
      canvas.height = video.offsetHeight
      canvas
        .getContext('2d')
        .drawImage(video, 0, 0, canvas.width, canvas.height)
      var data_url = canvas.toDataURL()

      return data_url
    }
    

    // main function to get video frame, convert it to blob, and send it to the server for processing
    function main() {
      console.log('Capturing and sending ...')
      var data_url = captureImage()
      var image_blob = dataURLtoBlob(data_url)
      //console.log(image_blob)
      socket.emit('detectFace', { data: image_blob,frame:0 })
    
      
    }

    // after every 800ms send frame to server
    // adjust it accordingly(depending on latency you are geting)
    setInterval(main, 1000)
    
  </script>
  
  </body>
</html>
